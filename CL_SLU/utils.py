#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sat May 14 16:14:13 2022

@author: umbertocappellazzo
"""
from torch.nn import functional as F
import math
import warnings
import torch
from torch import nn
from torchaudio import transforms
import numpy as np
import copy
import librosa
from scipy import signal

def trunc(x, max_len):
    l = len(x)
    if l > max_len:
        x = x[l//2-max_len//2:l//2+max_len//2]
    if l < max_len:
        x = F.pad(x, (0, max_len-l), value=0.)
    
    eps = np.finfo(np.float64).eps
    sample_rate = 16000
    n_mels = 40
    win_len = 25
    hop_len= 10
    win_len = int(sample_rate/1000*win_len)
    hop_len = int(sample_rate/1000*hop_len)
    mel_spectr = transforms.MelSpectrogram(sample_rate=16000,
            win_length=win_len, hop_length=hop_len, n_mels=n_mels)
    
    mel = mel_spectr(x)+eps
    #return mel_spectr(x)
    #return np.log(mel_spectr(x)+eps)  # Right one.
    #return x

def trunc_new(x,max_len=64000,win_len=0.02):
    eps = np.finfo(np.float64).eps
    sample_rate = 16000
    n_fft = int(win_len * sample_rate)
    l = len(x)
    if l > max_len:
        x = x[l//2-max_len//2:l//2+max_len//2]
    if l < max_len:
        x = F.pad(x, (0, max_len-l), value=0.)
    
    filters = librosa.filters.mel(sample_rate, n_fft, n_mels=40)
    window = signal.hamming(n_fft, sym=False)
    spectrogram = np.abs(
            librosa.stft(y=x.numpy() + eps, n_fft=n_fft, win_length=n_fft, hop_length=n_fft // 2, center=True, window=window))
    melspectrum = np.log(np.dot(filters, spectrogram) + eps)
    
    return torch.from_numpy(melspectrum)


    
    
    


    

def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    
    """Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.
    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)


def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def freeze_parameters(m, requires_grad=False):
    if m is None:
        return

    if isinstance(m, nn.Parameter):
        m.requires_grad = requires_grad
    else:
        for p in m.parameters():
            p.requires_grad = requires_grad




def drop_path(x, drop_prob: float = 0., training: bool = False, scale_by_keep: bool = True):
    """Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).
    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,
    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...
    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for
    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use
    'survival rate' as the argument.
    """
    if drop_prob == 0. or not training:
        return x
    keep_prob = 1 - drop_prob
    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets
    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)
    if keep_prob > 0.0 and scale_by_keep:
        random_tensor.div_(keep_prob)
    return x * random_tensor


class DropPath(nn.Module):
    """Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).
    """
    def __init__(self, drop_prob: float = 0., scale_by_keep: bool = True):
        super(DropPath, self).__init__()
        self.drop_prob = drop_prob
        self.scale_by_keep = scale_by_keep

    def forward(self, x):
        return drop_path(x, self.drop_prob, self.training, self.scale_by_keep)
    


class Memory:
    def __init__(self, memory_size, nb_total_classes, rehearsal, fixed=True):
        self.memory_size = memory_size
        self.nb_total_classes = nb_total_classes
        self.rehearsal = rehearsal
        self.fixed = fixed

        self.x = self.y = self.t = None

        self.nb_classes = 0

    @property
    def memory_per_class(self):
        if self.fixed:
            return self.memory_size // self.nb_total_classes
        return self.memory_size // self.nb_classes if self.nb_classes > 0 else self.memory_size

    def get_dataset(self, base_dataset):
        dataset = copy.deepcopy(base_dataset)
        dataset._x = self.x
        dataset._y = self.y
        dataset._t = self.t

        return dataset

    def get(self):
        return self.x, self.y, self.t

    def __len__(self):
        return len(self.x) if self.x is not None else 0

    def save(self, path):
        np.savez(
            path,
            x=self.x, y=self.y, t=self.t
        )

    def load(self, path):
        data = np.load(path)
        self.x = data["x"]
        self.y = data["y"]
        self.t = data["t"]

        assert len(self) <= self.memory_size, len(self)
        self.nb_classes = len(np.unique(self.y))

    def reduce(self):
        x, y, t = [], [], []
        for class_id in np.unique(self.y):
            indexes = np.where(self.y == class_id)[0]
            x.append(self.x[indexes[:self.memory_per_class]])
            y.append(self.y[indexes[:self.memory_per_class]])
            t.append(self.t[indexes[:self.memory_per_class]])

        self.x = np.concatenate(x)
        self.y = np.concatenate(y)
        self.t = np.concatenate(t)

    def add(self, dataset, model, nb_new_classes):
        self.nb_classes += nb_new_classes

        x, y, t = herd_samples(dataset, model, self.memory_per_class, self.rehearsal)
        #print(x.shape); print(y.shape)
        #assert len(y) == self.memory_per_class * nb_new_classes, (len(y), self.memory_per_class, nb_new_classes)

        if self.x is None:
            self.x, self.y, self.t = x, y, t
        else:
            if not self.fixed:
                self.reduce()
            self.x = np.concatenate((self.x, x))
            self.y = np.concatenate((self.y, y))
            self.t = np.concatenate((self.t, t))


def herd_samples(dataset, model, memory_per_class, rehearsal):
    x, y, t = dataset._x, dataset._y, dataset._t
    #print(x.shape); print(y.shape)

    if rehearsal == "random":
        indexes = []
        for class_id in np.unique(y):
            class_indexes = np.where(y == class_id)[0]
            indexes.append(
                np.random.choice(class_indexes, size=memory_per_class)
            )
        indexes = np.concatenate(indexes)

        return x[indexes], y[indexes], t[indexes]
    elif "closest" in rehearsal:
        if rehearsal == 'closest_token':
            handling = 'last'
        else:
            handling = 'all'

        features, targets = extract_features(dataset, model, handling)
        indexes = []

        for class_id in np.unique(y):
            class_indexes = np.where(y == class_id)[0]
            class_features = features[class_indexes]

            class_mean = np.mean(class_features, axis=0, keepdims=True)
            distances = np.power(class_features - class_mean, 2).sum(-1)
            class_closest_indexes = np.argsort(distances)

            indexes.append(
                class_indexes[class_closest_indexes[:memory_per_class]]
            )

        indexes = np.concatenate(indexes)
        return x[indexes], y[indexes], t[indexes]
    elif "furthest" in rehearsal:
        if rehearsal == 'furthest_token':
            handling = 'last'
        else:
            handling = 'all'

        features, targets = extract_features(dataset, model, handling)
        indexes = []

        for class_id in np.unique(y):
            class_indexes = np.where(y == class_id)[0]
            class_features = features[class_indexes]

            class_mean = np.mean(class_features, axis=0, keepdims=True)
            distances = np.power(class_features - class_mean, 2).sum(-1)
            class_furthest_indexes = np.argsort(distances)[::-1]

            indexes.append(
                class_indexes[class_furthest_indexes[:memory_per_class]]
            )

        indexes = np.concatenate(indexes)
        return x[indexes], y[indexes], t[indexes]
    elif "icarl":
        if rehearsal == 'icarl_token':
            handling = 'last'
        else:
            handling = 'all'

        features, targets = extract_features(dataset, model, handling)
        indexes = []

        for class_id in np.unique(y):
            class_indexes = np.where(y == class_id)[0]
            class_features = features[class_indexes]

            indexes.append(
                class_indexes[icarl_selection(class_features, memory_per_class)]
            )

        indexes = np.concatenate(indexes)
        return x[indexes], y[indexes], t[indexes]
    else:
        raise ValueError(f"Unknown rehearsal method {rehearsal}!")

def extract_features(dataset, model, ensemble_handling='last'):
    #transform = copy.deepcopy(dataset.trsf.transforms)
    #dataset.trsf = transforms.Compose(transform[-2:])

    loader = torch.utils.data.DataLoader(
        dataset,
        batch_size=128,
        num_workers=2,
        pin_memory=True,
        drop_last=False,
        shuffle=False
    )

    features, targets = [], []

    with torch.no_grad():
        for x, y, _ in loader:
            if hasattr(model, 'module'):
                feats, _, _ = model.module.forward_features(x.cuda())
            else:
                feats, _, _ = model.forward_features(x.cuda())

            if isinstance(feats, list):
                if ensemble_handling == 'last':
                    feats = feats[-1]
                elif ensemble_handling == 'all':
                    feats = torch.cat(feats, dim=1)
                else:
                    raise NotImplementedError(f'Unknown handdling of multiple features {ensemble_handling}')
            elif len(feats.shape) == 3:  # joint tokens
                if ensemble_handling == 'last':
                    feats = feats[-1]
                elif ensemble_handling == 'all':
                    feats = feats.permute(1, 0, 2).view(len(x), -1)
                else:
                    raise NotImplementedError(f'Unknown handdling of multiple features {ensemble_handling}')

            feats = feats.cpu().numpy()
            y = y.numpy()

            features.append(feats)
            targets.append(y)

    features = np.concatenate(features)
    targets = np.concatenate(targets)

    #dataset.trsf = transforms.Compose(transform)
    return features, targets



def icarl_selection(features, nb_examplars):
    D = features.T
    D = D / (np.linalg.norm(D, axis=0) + 1e-8)
    mu = np.mean(D, axis=1)
    herding_matrix = np.zeros((features.shape[0],))

    w_t = mu
    iter_herding, iter_herding_eff = 0, 0

    while not (
        np.sum(herding_matrix != 0) == min(nb_examplars, features.shape[0])
    ) and iter_herding_eff < 1000:
        tmp_t = np.dot(w_t, D)
        ind_max = np.argmax(tmp_t)
        iter_herding_eff += 1
        if herding_matrix[ind_max] == 0:
            herding_matrix[ind_max] = 1 + iter_herding
            iter_herding += 1

        w_t = w_t + mu - D[:, ind_max]

    herding_matrix[np.where(herding_matrix == 0)[0]] = 10000

    return herding_matrix.argsort()[:nb_examplars]


class InfiniteLoader:
    def __init__(self, loader):
        self.loader = loader
        self.reset()

    def reset(self):
        self.it = iter(self.loader)

    def get(self):
        try:
            return next(self.it)
        except StopIteration:
            self.reset()
            return self.get()


    
    
